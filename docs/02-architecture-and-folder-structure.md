# 02 â€” Architecture & Folder Structure (Monorepo)

## Overview

Arcanomy Motion is a **hybrid monorepo** combining:
1.  **Python Orchestrator:** Handles logic, LLM calls, and state management.
2.  **Remotion (React):** Handles deterministic rendering of video, charts, and typography.

We follow a **"Smart Agent + Dumb Scripts"** architecture. Agents orchestrate the flow by reading project files and combining prompts, while simple Python scripts execute single API calls (e.g., "generate one image", "render one video clip").

---

## ðŸ§ How It Works (Explain Like I'm 5)

Imagine you are making a LEGO movie. Here is how our robot factory does it:

### 1. The Idea (The Seed)
You tell the robot **"Make a movie about Bitcoin."**
You also give it a rules list: **"Use a serious man's voice"** and **"Make it 30 seconds long."**
*(This is `00_seed.md` and `00_reel.yaml`)*

### 2. The Writer (Story & Research)
The robot reads your idea. It goes to the library (Google) to check facts.
Then, it writes a script. But it doesn't just write paragraphs; it cuts the script into **10-second chunks**, like LEGO blocks.
*(This creates `02_story_generator.output.json`)*

> **Chunk 1:** "Bitcoin is digital gold." (Show a gold coin)
> **Chunk 2:** "It is scarce." (Show a chart going up)

### 3. The Artist (Image Gen)
For every chunk, the robot paints **one beautiful picture**.
If the chunk needs a chart, it draws a sketch of the chart.
*(This creates images in `renders/`)*

### 4. The Animator (Video Gen)
The robot takes that one picture and makes it move for **10 seconds**.
- It decides: "Zoom in slowly" or "Pan left."
- It uses a tool called **Kling** (or Runway) to turn the still picture into a moving video.
*(Now we have `bg_01.mp4`, `bg_02.mp4`...)*

### 5. The Voice Actor (Audio Gen)
The robot reads the script out loud using a tool called **ElevenLabs**.
It makes sure the voice lasts exactly as long as the video chunk.
*(Now we have `voice_01.mp3`, `voice_02.mp3`...)*

### 6. The Editor (Assembly)
Now the robot has a pile of stuff:
- 3 Video clips
- 3 Audio clips
- Background music

It glues them all together in a timeline. It puts the voice *on top* of the video, and the music *underneath* everything.
Finally, it presses "Print" (Render) and gives you **one final MP4 video file**.

---

## Root Layout

```text
arcanomy-motion/
â”œâ”€â”€ docs/                     # Project documentation
â”œâ”€â”€ src/                      # Python Source Code (Orchestrator)
â”œâ”€â”€ remotion/                 # Remotion Project (React/TypeScript)
â”œâ”€â”€ content/                  # User Data & Local Outputs (Git-ignored)
â”œâ”€â”€ shared/                   # Global Assets (fonts, logos, intro/outro)
â”œâ”€â”€ tests/                    # Python Tests
â”œâ”€â”€ pyproject.toml            # Python Dependencies & Config (Poetry/uv)
â””â”€â”€ .gitignore
```

---

## 1. `src/` (Python Orchestrator)

The brain of the system. It reads the inputs, calls APIs via scripts, and manages the detailed step-by-step pipeline.

```text
src/
â”œâ”€â”€ domain/                   # Data Classes & Types
â”‚   â”œâ”€â”€ objective.py          # Objective model & parsing logic
â”‚   â”œâ”€â”€ segment.py            # Segment definition
â”‚   â””â”€â”€ manifest.py           # The "Render Manifest" sent to Remotion
â”‚
â”œâ”€â”€ services/                 # External API Wrappers
â”‚   â”œâ”€â”€ llm.py                # OpenAI / Anthropic
â”‚   â”œâ”€â”€ elevenlabs.py         # Voice generation
â”‚   â””â”€â”€ remotion_cli.py       # Wrapper for calling `npx remotion render`
â”‚
â”œâ”€â”€ stages/                   # Pipeline Logic (Pure Python)
â”‚   â”œâ”€â”€ s01_research.py
â”‚   â”œâ”€â”€ s02_script.py
â”‚   â”œâ”€â”€ s03_plan.py
â”‚   â”œâ”€â”€ s04_assets.py
â”‚   â”œâ”€â”€ s05_assembly.py
â”‚   â””â”€â”€ s06_delivery.py
â”‚
â”œâ”€â”€ utils/                    # Helpers
â”‚   â”œâ”€â”€ io.py                 # Safe file reading/writing
â”‚   â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ main.py                   # Entry point (CLI)
â””â”€â”€ commands.py               # Click/Typer command definitions
```

---

## 2. `remotion/` (Visual Engine)

A standard Remotion project. It takes a JSON payload (the "Timeline" or "Manifest") and renders pixels. It does **not** make decisions.

```text
remotion/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ compositions/         # Top-level Entry Points
â”‚   â”‚   â”œâ”€â”€ MainReel.tsx      # The primary composition
â”‚   â”‚   â””â”€â”€ Shorts.tsx
â”‚   â”‚
â”‚   â”œâ”€â”€ components/           # Reusable UI
â”‚   â”‚   â”œâ”€â”€ charts/           # D3/Visx wrappers
â”‚   â”‚   â”œâ”€â”€ typography/       # Text animations
â”‚   â”‚   â””â”€â”€ layouts/          # Screen arrangements (e.g., SplitScreen)
â”‚   â”‚
â”‚   â”œâ”€â”€ lib/                  # Helpers
â”‚   â”‚   â”œâ”€â”€ load-fonts.ts
â”‚   â”‚   â””â”€â”€ utils.ts
â”‚   â”‚
â”‚   â””â”€â”€ Root.tsx              # Remotion registration
â”‚
â”œâ”€â”€ public/                   # Static assets for preview
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

---

## 3. `content/` (User Data & Artifacts)

This is where the magic happens. We follow a granular, step-by-step file structure where every stage produces an explicit **input prompt** and **output result**. This allows us to inspect, debug, and manually intervene at any point.

**Structure per Reel:**

```text
content/
â””â”€â”€ reels/
    â””â”€â”€ 2024-05-20-sunk-cost/        # Unique Reel Slug
        â”œâ”€â”€ 00_seed.md               # [Input] The initial user concept/brief
        â”œâ”€â”€ 00_reel.yaml             # [Input] Machine settings (voice, music, etc.)
        â”œâ”€â”€ 00_data/                 # [Input] Local CSVs for charts
        â”‚   â””â”€â”€ trading.csv
        â”‚
        â”œâ”€â”€ 01_research.input.md     # [Gen] Exact prompt sent to research agent
        â”œâ”€â”€ 01_research.output.md    # [Gen] Research findings
        â”‚
        â”œâ”€â”€ 02_story_generator.input.md
        â”œâ”€â”€ 02_story_generator.output.md
        â”œâ”€â”€ 02_story_generator.output.json  # [Key] Segmentation source of truth
        â”‚
        â”œâ”€â”€ 03_character_generation.input.md
        â”œâ”€â”€ 03_character_generation.output.md
        â”‚
        â”œâ”€â”€ 03.5_generate_assets_agent.input.md
        â”œâ”€â”€ 03.5_generate_assets_agent.output.json # Asset list (images/charts)
        â”‚
        â”œâ”€â”€ 04_video_prompt_engineering.input.md
        â”œâ”€â”€ 04_video_prompt_engineering.output.md
        â”‚
        â”œâ”€â”€ 04.5_generate_videos_agent.input.md
        â”œâ”€â”€ 04.5_generate_videos_agent.output.json # Video file paths
        â”‚
        â”œâ”€â”€ 05_voice_prompt_engineer.input.md
        â”œâ”€â”€ 05_voice_prompt_engineer.output.md
        â”‚
        â”œâ”€â”€ 05.5_generate_audio_agent.input.md
        â”œâ”€â”€ 05.5_generate_audio_agent.output.json # Audio file paths
        â”‚
        â”œâ”€â”€ 06_music.input.md
        â”œâ”€â”€ 06_music.output.json
        â”‚
        â”œâ”€â”€ 07_assemble_final_agent.input.md
        â”œâ”€â”€ 07_assemble_final_agent.output.json # Final Remotion timeline
        â”‚
        â”œâ”€â”€ renders/                 # [Assets] Intermediate media files
        â”‚   â”œâ”€â”€ bg_01.mp4
        â”‚   â”œâ”€â”€ bg_02.mp4
        â”‚   â”œâ”€â”€ intro.png
        â”‚   â”œâ”€â”€ voice_full.mp3
        â”‚   â””â”€â”€ charts/              # Remotion chart renders (optional/cache)
        â”‚
        â””â”€â”€ final/                   # [Delivery] Final outputs
            â”œâ”€â”€ final.mp4
            â”œâ”€â”€ final.srt
            â””â”€â”€ metadata.json
```

### Philosophy: Explicit State

- **Inputs (.input.md):** We save the *exact* prompt sent to the LLM. This allows us to tweak the prompt manually if needed and re-run the step.
- **Outputs (.output.md/.json):** We save the text response (MD) and the structured data (JSON). The JSON is often the input for the next "dumb script" (e.g., a list of image prompts to generate).
- **Renders Folder:** All heavy media files live in `renders/` to keep the root directory text-focused and scannable.

---

## 4. `shared/` (Global Assets)

Assets and configuration that apply to *all* reels (brand identity and agent instructions).

```text
shared/
â”œâ”€â”€ fonts/
â”œâ”€â”€ logos/
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ intro.mp3
â”‚   â””â”€â”€ watermark.mp3
â”œâ”€â”€ prompts/                  # System prompts for agents (reusable instructions)
â”‚   â”œâ”€â”€ research_system.md    # Research assistant instructions
â”‚   â”œâ”€â”€ script_system.md      # Scriptwriter instructions
â”‚   â”œâ”€â”€ visual_plan_system.md # Visual director instructions
â”‚   â”œâ”€â”€ assets_system.md      # Image/video prompt engineer instructions
â”‚   â””â”€â”€ voice_system.md       # Voice director instructions
â””â”€â”€ templates/                # User-facing templates (starting points)
    â””â”€â”€ seed_template.md      # Template for creating new reels
```

**Distinction:**
- **`prompts/`**: System prompts define agent behavior and are loaded by the orchestrator. These are internal instructions that apply globally across all reels.
- **`templates/`**: User-facing templates provide formats/starting points for users to fill in (e.g., `seed_template.md`).

---

## Workflow Summary

1.  **User** creates `00_seed.md` and `00_reel.yaml`.
2.  **Orchestrator** runs step-by-step:
    -   Reads inputs.
    -   Generates `.input.md` (prompt).
    -   Calls Agent/LLM.
    -   Saves `.output.md` and `.output.json`.
    -   Calls "Dumb Scripts" (using the JSON) to generate assets into `renders/`.
3.  **Final Assembly** reads all JSONs and assets, creates a timeline, and calls Remotion to render the final video.
